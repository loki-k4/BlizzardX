{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_inventory_txt = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n",
    "url_stations_txt = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "url_countries_txt = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt\"\n",
    "url_states_txt = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-states.txt\"\n",
    "url_state_raw = \"https://raw.githubusercontent.com/georgique/world-geojson/develop/states/usa/new_hampshire.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stations_txt(url):\n",
    "    r = requests.get(url)\n",
    "    lines = r.text.split(\"\\n\")\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            data.append({\n",
    "                \"ID\": line[0:11].strip(),\n",
    "                \"LATITUDE\": float(line[12:20].strip()),\n",
    "                \"LONGITUDE\": float(line[21:30].strip()),\n",
    "                \"ELEVATION\": float(line[31:37].strip()),\n",
    "                \"STATE\": line[38:40].strip(),\n",
    "                \"NAME\": line[38:68].strip(),\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_inventory_txt(url):\n",
    "    r = requests.get(url)\n",
    "    lines = r.text.split(\"\\n\")\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            data.append({\n",
    "                \"ID\": line[0:11].strip(),\n",
    "                \"LATITUDE\": float(line[12:20].strip()),\n",
    "                \"LONGITUDE\": float(line[21:30].strip()),\n",
    "                \"ELEMENT\": line[31:35].strip(),\n",
    "                \"FIRSTYEAR\": int(line[36:40].strip()),\n",
    "                \"LASTYEAR\": int(line[41:45].strip())\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_countries_txt(url):\n",
    "    r = requests.get(url)\n",
    "    lines = r.text.split(\"\\n\")\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            data.append({\n",
    "                \"CODE\": line[0:2].strip(),\n",
    "                \"NAME\": line[3:64].strip()\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_states_txt(url):\n",
    "    r = requests.get(url)\n",
    "    lines = r.text.split(\"\\n\")\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            data.append({\n",
    "                \"CODE\": line[0:2].strip(),\n",
    "                \"NAME\": line[3:50].strip()\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def parse_data_dly(line):\n",
    "    data = []\n",
    "    for i in range(21, 269, 8):\n",
    "        value = int(line[i:i+5])/10\n",
    "        mflag = line[i+5]\n",
    "        qflag = line[i+6]\n",
    "        sflag = line[i+7]\n",
    "        data.extend([value, mflag, qflag, sflag])\n",
    "    return {\n",
    "        \"ID\": line[0:11].strip(),\n",
    "        \"YEAR\": int(line[11:15]),\n",
    "        \"Month\": int(line[15:17]),\n",
    "        \"ELEMENT\": line[17:21].strip(),\n",
    "        \"DATA\": data\n",
    "    }\n",
    "\n",
    "def read_data_from_url(url):\n",
    "    data = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        for line in response.text.splitlines():\n",
    "            data.append(parse_data_dly(line))\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {url}. Status code: {response.status_code}\")\n",
    "    return data\n",
    "\n",
    "def fetch_and_save_to_dataframe(station_ids):\n",
    "    all_data = []\n",
    "    for station_id in tqdm(station_ids, desc=\"Fetching Data\", unit=\"station\", ncols=100):\n",
    "        url = f\"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/{station_id}.dly\"\n",
    "        data = read_data_from_url(url)\n",
    "        all_data.extend(data)\n",
    "    headers = [\"ID\", \"YEAR\", \"Month\", \"ELEMENT\"]\n",
    "    for i in range(1, 32):\n",
    "        headers.extend([f\"VALUE{i}\", f\"MFLAG{i}\", f\"QFLAG{i}\", f\"SFLAG{i}\"])\n",
    "    df_data = []\n",
    "    for entry in all_data:\n",
    "        row = [entry[\"ID\"], entry[\"YEAR\"], entry[\"Month\"], entry[\"ELEMENT\"]]\n",
    "        row.extend(entry[\"DATA\"])\n",
    "        df_data.append(row)\n",
    "    return pd.DataFrame(df_data, columns=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = parse_inventory_txt(url_inventory_txt)\n",
    "stations= parse_stations_txt(url_stations_txt)\n",
    "countries = parse_countries_txt(url_countries_txt)\n",
    "states = parse_states_txt(url_states_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For country\n",
    "#s_country_list = stations[stations['ID'].str.startswith('US')]['ID'].tolist()\n",
    "# For state\n",
    "s_state_list=stations[stations['STATE']=='NH']['ID'].tolist()\n",
    "s_live_list=inventory[(inventory['ID'].isin(s_state_list)) & (inventory['LASTYEAR']>=2024)]['ID'].unique().tolist()\n",
    "result = inventory.loc[inventory['ID'].isin(s_live_list), ['ID', 'ELEMENT', 'FIRSTYEAR', 'LASTYEAR']]\n",
    "result['YEAR_DIFF'] = result['LASTYEAR'] - result['FIRSTYEAR']\n",
    "result_filtered = result[result['ELEMENT'].isin([ 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'PRCP'])]\n",
    "Req_station_list = pd.Series(result_filtered[(result_filtered['YEAR_DIFF'] >= 70) & (result_filtered['LASTYEAR'] > 2024)]['ID'].tolist()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= fetch_and_save_to_dataframe(Req_station_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_columns = [col for col in data.columns if 'FLAG' in col]\n",
    "data= data.drop(columns=flag_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather_data(df):\n",
    "    transformed_data = {}\n",
    "    for index, row in df.iterrows():\n",
    "        for day in range(1, 32):\n",
    "            date = pd.to_datetime(f\"{row['YEAR']}-{row['Month']:02d}-{day:02d}\", errors='coerce')\n",
    "            if pd.notna(date):\n",
    "                if date not in transformed_data:\n",
    "                    transformed_data[date] = {\"ID\": row['ID']}\n",
    "                element = row['ELEMENT']\n",
    "                value = row.get(f'VALUE{day}', None)\n",
    "                if pd.notna(value) and value != -999.9:\n",
    "                    transformed_data[date][element] = value\n",
    "    return pd.DataFrame.from_dict(transformed_data, orient='index').reset_index().rename(columns={'index': 'DATE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_stations = stations[stations['STATE']=='NH']\n",
    "state_lat = state_stations['LATITUDE'].mean()\n",
    "state_lon = state_stations['LONGITUDE'].mean()\n",
    "state_map = folium.Map(location=[state_lat, state_lon], \n",
    "                       zoom_start=7.5,\n",
    "                       min_zoom=6,\n",
    "                       max_zoom=10)\n",
    "\n",
    "stations_to_plot = stations[stations['ID'].isin(Req_station_list)]\n",
    "for _, row in stations_to_plot.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        popup=row['NAME'],\n",
    "        tooltip=row['NAME']\n",
    "    ).add_to(state_map)\n",
    "try:\n",
    "    geojson_data = requests.get(url_state_raw).json() \n",
    "    folium.GeoJson(geojson_data, name=\"State Boundary\").add_to(state_map)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GeoJSON: {e}\")\n",
    "    \n",
    "state_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "# Apply the function to the 'month' column\n",
    "data['season'] = data['Month'].apply(get_season)\n",
    "\n",
    "# Now you can split the data by season\n",
    "winter_data = data[data['season'] == 'Winter']\n",
    "spring_data = data[data['season'] == 'Spring']\n",
    "summer_data = data[data['season'] == 'Summer']\n",
    "fall_data = data[data['season'] == 'Fall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of accessing the split data\n",
    "winter_data[(winter_data['YEAR'] == 2024 )& (winter_data['Month']==2)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'station' and 'element', then count occurrences of each element per station\n",
    "element_counts_by_station = data.groupby(['ID', 'ELEMENT']).size().unstack(fill_value=0)\n",
    "\n",
    "# Print the result\n",
    "element_counts_by_station[['SNOW', 'SNWD', 'WESD', 'PRCP', 'TMAX', 'TMIN', 'ADPT']].sum(axis=1).sort_values(ascending=False).head(10)\n",
    "\n",
    "# Individual counts for each element\n",
    "element_counts_by_station[['SNOW', 'SNWD', 'WESD', 'PRCP', 'TMAX', 'TMIN', 'ADPT']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "data_unique = data.drop_duplicates(subset=['ID', 'YEAR', 'Month'])\n",
    "\n",
    "# Group by 'ID' and count the number of rows for each station\n",
    "rows_per_station = data_unique.groupby('ID').size()\n",
    "\n",
    "print(rows_per_station)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
